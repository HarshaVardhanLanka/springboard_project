{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN87GGVIFVz2X/uMhs+D/7v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarshaVardhanLanka/OpenDeepResearchAgent/blob/main/Day_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In LangGraph, the Searcher is a standalone module. If the Searcher fails, the Planner and Writer don't care‚Äîthey just wait for the state to update.**"
      ],
      "metadata": {
        "id": "GqFVIvY_v1Fx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Phase 1: The Setup**\n",
        "\n",
        "We need to install langgraph and the official LangChain integration for Gemini (langchain-google-genai)"
      ],
      "metadata": {
        "id": "l-F1yPTJv6Qm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zl4KW1xtqbk1"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U langgraph langchain langchain-google-genai tavily-python pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Phase 2: Config & State Definition**\n",
        "\n",
        "Here we define the \"Shared Memory\" (State) that will be passed between agents."
      ],
      "metadata": {
        "id": "qgWjHYw_wC7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from typing import TypedDict, List\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# LangChain Imports\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from tavily import TavilyClient\n",
        "\n",
        "# 1. Setup API Keys\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    TAVILY_API_KEY = userdata.get('TAVILY') # Note: Ensure this matches your Secret name\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è Error: Please set GOOGLE_API_KEY and TAVILY in Colab Secrets.\")\n",
        "\n",
        "# 2. Initialize the LLM\n",
        "# We use 'gemini-1.5-flash' as it is fast and efficient for agent workflows\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro-latest\", temperature=0)\n",
        "\n",
        "# 3. Initialize Tavily\n",
        "tavily = TavilyClient(api_key=TAVILY_API_KEY)\n",
        "\n",
        "print(\"Setup complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vThoHruJqcnu",
        "outputId": "1f24068d-2cea-4efa-c4b7-b1fa5c950cfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DEFINE THE STATE (The \"Clipboard\"):**\n",
        "\n",
        " This is the data structure passed between agents"
      ],
      "metadata": {
        "id": "AYLnExpR1mBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. DEFINE THE STATE (The \"Clipboard\")\n",
        "# This is the data structure passed between agents\n",
        "class AgentState(TypedDict):\n",
        "    topic: str                # The User's input (or PDF content)\n",
        "    research_plan: List[str]  # Output from Planner\n",
        "    search_results: str       # Output from Searcher\n",
        "    final_report: str         # Output from Writer"
      ],
      "metadata": {
        "id": "PlNptSCfq-dL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Memory System**"
      ],
      "metadata": {
        "id": "4Mn_VoO8wqe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import datetime\n",
        "\n",
        "# --- MEMORY SYSTEM ---\n",
        "class HistoryManager:\n",
        "    def __init__(self):\n",
        "        self.history_file = \"agent_history.json\"\n",
        "        self.history = self.load_history()\n",
        "\n",
        "    def load_history(self):\n",
        "        \"\"\"Loads history from a JSON file if it exists.\"\"\"\n",
        "        if os.path.exists(self.history_file):\n",
        "            with open(self.history_file, 'r') as f:\n",
        "                return json.load(f)\n",
        "        return []\n",
        "\n",
        "    def save_entry(self, input_text, mode, final_report):\n",
        "        \"\"\"Saves a new research session to memory.\"\"\"\n",
        "        entry = {\n",
        "            \"id\": len(self.history) + 1,\n",
        "            \"timestamp\": str(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")),\n",
        "            \"mode\": mode,\n",
        "            \"input\": input_text[:50] + \"...\" if len(input_text) > 50 else input_text, # Preview only\n",
        "            \"full_input\": input_text,\n",
        "            \"report\": final_report\n",
        "        }\n",
        "        self.history.append(entry)\n",
        "        with open(self.history_file, 'w') as f:\n",
        "            json.dump(self.history, f)\n",
        "        print(\"‚úÖ Research saved to History.\")\n",
        "\n",
        "    def show_history(self):\n",
        "        \"\"\"Displays list of past inputs.\"\"\"\n",
        "        if not self.history:\n",
        "            print(\"\\nüì≠ History is empty.\")\n",
        "            return None\n",
        "\n",
        "        print(\"\\nüìö RESEARCH HISTORY:\")\n",
        "        print(f\"{'ID':<4} | {'Time':<18} | {'Mode':<8} | {'Topic/Input'}\")\n",
        "        print(\"-\" * 60)\n",
        "        for item in self.history:\n",
        "            print(f\"{item['id']:<4} | {item['timestamp']:<18} | {item['mode']:<8} | {item['input']}\")\n",
        "        return self.history\n",
        "# Initialize Memory\n",
        "memory = HistoryManager()"
      ],
      "metadata": {
        "id": "jj393Qevsoab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Phase 3: The Agents (Nodes)**\n",
        "\n",
        "We will define your three agents as specific functions. Each function receives the state, does work, and returns an update to the state."
      ],
      "metadata": {
        "id": "_oG_JwtTwK3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. The Planner Agent**\n",
        "\n",
        "Breaks the topic down."
      ],
      "metadata": {
        "id": "tYXm_Y_KwTJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def planner_node(state: AgentState):\n",
        "    print(f\"üß† [Planner]: Analying request...\")\n",
        "    topic = state['topic']\n",
        "\n",
        "    # We ask the LLM to return a clean list\n",
        "    prompt = f\"\"\"\n",
        "    You are a Research Planner.\n",
        "    Topic/Context: {topic}\n",
        "\n",
        "    Task: Generate 3 distinct, specific search queries to gather comprehensive information.\n",
        "    Constraint: Return ONLY the 3 queries separated by newlines. Do not number them.\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "\n",
        "    # Process output into a list\n",
        "    queries = [q.strip() for q in response.content.split('\\n') if q.strip()]\n",
        "    print(f\"   -> Generated Plan: {queries[:3]}\")\n",
        "\n",
        "    return {\"research_plan\": queries[:3]}"
      ],
      "metadata": {
        "id": "jq_6VwkRrL40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. The Searcher Agent**\n",
        "\n",
        "Executes the plan."
      ],
      "metadata": {
        "id": "KtEYi3ICwdux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def searcher_node(state: AgentState):\n",
        "    print(f\"üîé [Searcher]: Executing research plan...\")\n",
        "    queries = state['research_plan']\n",
        "    results = []\n",
        "\n",
        "    for q in queries:\n",
        "        print(f\"   - Searching: {q}\")\n",
        "        try:\n",
        "            # search_depth=\"advanced\" gives better quality for reports\n",
        "            response = tavily.search(query=q, max_results=1, search_depth=\"basic\")\n",
        "            if response['results']:\n",
        "                content = response['results'][0]['content']\n",
        "                results.append(f\"Source: {q}\\nContent: {content}\")\n",
        "            else:\n",
        "                results.append(f\"Source: {q}\\nContent: No data found.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error searching {q}: {e}\")\n",
        "\n",
        "    # Combine all results into one string\n",
        "    combined_content = \"\\n\\n\".join(results)\n",
        "    return {\"search_results\": combined_content}"
      ],
      "metadata": {
        "id": "QFcq2E7yrO4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. The Writer Agent**\n",
        "\n",
        "Synthesizes the report"
      ],
      "metadata": {
        "id": "2A0Y_7KPw5kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def writer_node(state: AgentState):\n",
        "    print(f\"‚úçÔ∏è [Writer]: Synthesizing final report...\")\n",
        "\n",
        "    topic = state['topic']\n",
        "    data = state['search_results']\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are a Senior Technical Writer.\n",
        "\n",
        "    Original Topic/Context: {topic[:500]}... (truncated for brevity)\n",
        "\n",
        "    Verified Research Data:\n",
        "    {data}\n",
        "\n",
        "    Task: Write a professional, structured summary.\n",
        "    1. Introduction\n",
        "    2. Key Findings (incorporate the research data)\n",
        "    3. Conclusion\n",
        "\n",
        "    Format: Markdown.\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    return {\"final_report\": response.content}"
      ],
      "metadata": {
        "id": "s6JKugVhrSKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Phase 4: The Execution Pipeline (Graph)**\n",
        "\n",
        "This is where we wire the agents together using LangGraph."
      ],
      "metadata": {
        "id": "Ong2ID3WxE6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Initialize Graph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# 2. Add Nodes\n",
        "workflow.add_node(\"planner\", planner_node)\n",
        "workflow.add_node(\"searcher\", searcher_node)\n",
        "workflow.add_node(\"writer\", writer_node)\n",
        "\n",
        "# 3. Define Edges (The Flow)\n",
        "workflow.set_entry_point(\"planner\")       # User Input -> Planner\n",
        "workflow.add_edge(\"planner\", \"searcher\")  # Planner -> Searcher\n",
        "workflow.add_edge(\"searcher\", \"writer\")   # Searcher -> Writer\n",
        "workflow.add_edge(\"writer\", END)          # Writer -> Finish\n",
        "\n",
        "# 4. Compile\n",
        "app = workflow.compile()\n",
        "print(\"‚úÖ Multi-Agent System Compiled successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxFz8HGYrVo8",
        "outputId": "e815cbd8-7532-4ed6-8bb8-318e8471a52c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Multi-Agent System Compiled successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Phase 5: Main Execution Loop (Integration)**\n",
        "\n",
        "We will keep your existing PDF and History logic, but now we feed the data into the Graph (app.invoke) instead of calling functions manually."
      ],
      "metadata": {
        "id": "1nYzGG9nxNP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import pypdf\n",
        "from google.colab import files\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# --- HELPER: PDF EXTRACTOR ---\n",
        "def extract_pdf_text(uploaded_file):\n",
        "    pdf_reader = pypdf.PdfReader(io.BytesIO(uploaded_file))\n",
        "    text = \"\"\n",
        "    for page in pdf_reader.pages:\n",
        "        text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "# --- MAIN APP ---\n",
        "def main():\n",
        "    print(\"ü§ñ LANGGRAPH RESEARCH AGENT INITIALIZED\")\n",
        "\n",
        "    while True:\n",
        "        print(\"\\n\" + \"=\"*15 + \" STRICT HYBRID AGENT \" + \"=\"*15)\n",
        "        print(\"1. Research a Topic (Text üí¨)\")\n",
        "        print(\"2. Research a Paper (Upload PDF üìñ)\")\n",
        "        print(\"3. View History üìú\")\n",
        "        print(\"4. Exit üì§\")\n",
        "\n",
        "        choice = input(\"Select Option: \")\n",
        "\n",
        "        if choice == '4':\n",
        "            print(\"Goodbye.üëã\")\n",
        "            break\n",
        "\n",
        "        # --- OPTION 3: HISTORY ---\n",
        "        if choice == '3':\n",
        "            history_data = memory.show_history()\n",
        "            if history_data:\n",
        "                sub_choice = input(\"\\nEnter ID to view full report (or Press Enter to go back): \")\n",
        "                if sub_choice.isdigit():\n",
        "                    idx = int(sub_choice) - 1\n",
        "                    if 0 <= idx < len(history_data):\n",
        "                        entry = history_data[idx]\n",
        "                        print(f\"\\nüîÅ RECALLING REPORT FOR: {entry['input']}\")\n",
        "                        print(\"-\" * 50)\n",
        "                        display(Markdown(entry['report']))\n",
        "                        input(\"\\nPress Enter to continue...\")\n",
        "            continue # Skip the rest and go back to menu\n",
        "\n",
        "        # SETUP INPUT\n",
        "        mode = \"\"\n",
        "        input_data = \"\"\n",
        "        label = \"\" # Used for history title\n",
        "\n",
        "        # Prepare Input\n",
        "        user_input = \"\"\n",
        "\n",
        "        if choice == '1':\n",
        "            user_input = input(\"Enter Topic: \")\n",
        "            mode = \"Text\"\n",
        "            label = user_input\n",
        "        elif choice == '2':\n",
        "            print(\"Upload PDF:\")\n",
        "            uploaded = files.upload()\n",
        "            if uploaded:\n",
        "                fn = next(iter(uploaded))\n",
        "                raw_text = extract_pdf_text(uploaded[fn])\n",
        "                # We wrap the PDF text in a prompt so the Planner knows what to do\n",
        "                user_input = f\"Analyze and validate this paper content: {raw_text[:10000]}\"\n",
        "                mode = \"PDF\"\n",
        "                label = fn\n",
        "            else:\n",
        "                print(\"No file.\")\n",
        "                continue\n",
        "        else:\n",
        "            print(\"Invalid option. Please try again.\")\n",
        "            continue\n",
        "\n",
        "        if user_input:\n",
        "            print(\"\\nüöÄ STARTING AGENT PIPELINE...\\n\")\n",
        "\n",
        "            # --- THE MAGIC HAPPENS HERE ---\n",
        "            # We invoke the LangGraph application\n",
        "            final_state = app.invoke({\"topic\": user_input})\n",
        "\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"üìä FINAL REPORT\")\n",
        "            print(\"=\"*50 + \"\\n\")\n",
        "            display(Markdown(final_state['final_report']))\n",
        "            memory.save_entry(label, mode, final_state['final_report'])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Mb3BSnykraoh",
        "outputId": "4c0a65f0-92cc-456a-c773-b199d8cc4012"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ LANGGRAPH RESEARCH AGENT INITIALIZED\n",
            "\n",
            "=============== STRICT HYBRID AGENT ===============\n",
            "1. Research a Topic (Text üí¨)\n",
            "2. Research a Paper (Upload PDF üìñ)\n",
            "3. View History üìú\n",
            "4. Exit üì§\n",
            "Select Option: 2\n",
            "Upload PDF:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-20eb7cec-82ad-4643-ab2f-0c7cfacec274\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-20eb7cec-82ad-4643-ab2f-0c7cfacec274\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 2511.17546v1.pdf to 2511.17546v1 (2).pdf\n",
            "\n",
            "üöÄ STARTING AGENT PIPELINE...\n",
            "\n",
            "üß† [Planner]: Analying request...\n",
            "   -> Generated Plan: ['performance and energy resolution of cryogenic microcalorimeters for 10-40 keV x-ray spectroscopy', 'theoretical calculations of hyperfine splitting and nuclear polarization in muonic atoms Z=23-30', 'accuracy of ab initio electric field gradient (EFG) calculations for open d-shell atoms like copper and cobalt']\n",
            "üîé [Searcher]: Executing research plan...\n",
            "   - Searching: performance and energy resolution of cryogenic microcalorimeters for 10-40 keV x-ray spectroscopy\n",
            "   - Searching: theoretical calculations of hyperfine splitting and nuclear polarization in muonic atoms Z=23-30\n",
            "   - Searching: accuracy of ab initio electric field gradient (EFG) calculations for open d-shell atoms like copper and cobalt\n",
            "‚úçÔ∏è [Writer]: Synthesizing final report...\n",
            "\n",
            "==================================================\n",
            "üìä FINAL REPORT\n",
            "==================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Of course. Here is a professional, structured summary based on the provided information.\n\n***\n\n### **Technical Summary: Determining Nuclear Quadrupole Moments via Muonic Atom Spectroscopy**\n\n**Paper:** \"Reference Quadrupole Moments of Transition Elements from Lamb Shifts in Muonic Atoms\" by S. Rathi et al.\n\n---\n\n### 1. Introduction\n\nThe paper by S. Rathi et al. presents a methodology for the high-precision determination of nuclear electric quadrupole moments for transition elements. The approach leverages the unique properties of muonic atoms, where a muon replaces an electron. Due to the muon's significantly larger mass, it orbits much closer to the nucleus, making its energy levels exceptionally sensitive to nuclear structure, size, and shape. By precisely measuring the Lamb shift‚Äîthe energy difference between specific atomic states‚Äîthe authors extract reference values for the nuclear quadrupole moment, a fundamental parameter describing the deviation of the nuclear charge distribution from a perfect sphere.\n\n### 2. Key Findings\n\nThe study's conclusions are substantiated by a combination of advanced experimental techniques and robust theoretical calculations, which are validated against existing research in the field.\n\n*   **Advanced Experimental Technique for X-ray Spectroscopy**\n    The determination of the Lamb shift relies on measuring the energy of X-rays emitted as the muon cascades down through atomic energy levels. The required precision necessitates state-of-the-art detection technology. This is supported by research into **high-resolution x-ray energy-dispersive spectrometers (EDS) based on cryogenic microcalorimeters**. These detectors provide the exceptional energy resolution needed to resolve subtle energy shifts in the 10-40 keV X-ray range, which is characteristic of muonic atom transitions. This technology is fundamental to achieving the experimental accuracy reported in the paper.\n\n*   **Robust Theoretical Framework and Validation**\n    Extracting the quadrupole moment from the measured Lamb shift requires a comprehensive theoretical model that accounts for all significant quantum electrodynamics (QED), nuclear polarization, and relativistic effects. The validity of these complex models is often benchmarked against other measurable nuclear-atomic interactions. This is consistent with **theoretical calculations of hyperfine splitting in muonic atoms (Z=23-30)**. The availability of both theoretical and experimental data on hyperfine splitting provides a crucial cross-check, ensuring the underlying models used to interpret the Lamb shift are accurate and reliable for this specific range of transition elements.\n\n*   **Addressing Computational Challenges in EFG Calculations**\n    A critical component of the analysis is the calculation of the Electric Field Gradient (EFG) at the nucleus, as the measured energy shift is directly proportional to the product of the EFG and the quadrupole moment. For transition elements with open d-shells (like copper and cobalt), *ab initio* EFG calculations are notoriously complex and a significant source of uncertainty. The paper's reliance on accurate EFG values is highlighted by ongoing research focused on the **accuracy of EFG calculations for open d-shell atoms**. Studies investigating methods like hybrid DFT functionals to improve EFG accuracy are essential for reducing the final uncertainty in the quadrupole moments derived by Rathi et al.\n\n### 3. Conclusion\n\nThe paper by Rathi et al. makes a significant contribution by providing new, high-precision reference values for the nuclear quadrupole moments of transition elements. The work successfully demonstrates the power of muonic atom spectroscopy as a clean and sensitive probe of nuclear structure. The findings are underpinned by the synergy between cutting-edge experimental capabilities, such as cryogenic microcalorimeter-based spectroscopy, and sophisticated, well-validated theoretical frameworks. This research not only enhances our fundamental understanding of nuclear physics but also provides benchmark data essential for refining nuclear models."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Research saved to History.\n",
            "\n",
            "=============== STRICT HYBRID AGENT ===============\n",
            "1. Research a Topic (Text üí¨)\n",
            "2. Research a Paper (Upload PDF üìñ)\n",
            "3. View History üìú\n",
            "4. Exit üì§\n",
            "Select Option: 4\n",
            "Goodbye.üëã\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Summary of Changes**\n",
        "\n",
        "***State Object:*** Instead of independent variables, we now use AgentState. This makes debugging easier (you can print the state at any point to see exactly what the agent knows).\n",
        "\n",
        "\n",
        "***Nodes vs Functions:*** The logic is encapsulated in \"Nodes\". If you wanted to replace Tavily with Google Search, you only edit the searcher_node; the rest of the system remains untouched.\n",
        "\n",
        "***Scalability:*** This architecture allows you to easily add loops (e.g., \"If the Writer isn't happy, send back to Searcher\") in future milestones."
      ],
      "metadata": {
        "id": "OvFdMpmLywph"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZUIqdiXMrgWv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}